---
title: 北京市租房信息分析
date: 2022-05-07
permalink: /blogs/2022/05/07/blog-post-1
tags:
  - 爬虫
  - 数据分析
  - 神经网络
---

中国社科院19年发布的《社会蓝皮书》指出，2019年我国已进入非农从业人员中白领总量超过蓝领的“白领时代”，对车、房、服务等的需求将不断提升。目前租房市场租房者近七成是 85 后，近半为 90 后 (95 后也将逐步进入职场)，其中很多人是“空巢青年”。

------

数据说明
======

数据的获取渠道为背景[链家网](https://bj.lianjia.com/)。 链家网是集房源信息搜索、产品研发、大数据处理、服务标准建立为一体的国内领先且重度垂直的全产业链房产服务平台。

从北京链家网的租房信息中可以看到许多相关信息，比如房屋地址，面积，租金等等信息，综合已有数据和北京市当前租房现状，我们采集的数据项为以下几个：

  * ①房屋名称；
  * ②房屋所在市区名称；
  * ③房屋具体地址；
  * ④房屋面积；
  * ⑤房屋朝向；
  * ⑥房屋房型；
  * ⑦是否靠近地铁；
  * ⑧房屋月租金；

数据集可在[这里找到](../static/北京市租房信息分析/北京链家网租房数据.xlsx)。

------

报告文档下载
======

在此下载[详细报告PDF](../static/北京市租房信息分析/北京市租房信息分析.pdf)。

------

Python代码
======

分析代码可在PDF中找到，这里给出数据采集代码：

```python
import re
import time
import requests
import pandas as pd
from random import uniform

from bs4 import BeautifulSoup

# 北京共有17个区，选取GDP排名靠前的7个区做租房信息，其余10个区的租房信息相对较少，无法获取有效的租房信息
regions = ['haidian', 'chaoyang', 'xicheng', 'dongcheng', 'shunyi', 'fengtai', 'daxing']

# 构建正则匹配规则
findName = re.compile(r'target="_blank" title="(.*)">')
findDistrict = re.compile(r'>(.*?)</a>-<a ')
findLocation = re.compile(r'target="_blank">(.*?)</a>-<a')
findArea = re.compile(r'(\d*\.\d*)㎡')
findDirection = re.compile(r'<i>/</i>(.*?) {8}<i>/</i>')
findHousetype = re.compile(r'(\d.\d?.?\d?.?) {8}<span class="hide">')
findSubway = re.compile(r'<i class="content__item__tag--is_subway_house">(.*)</i>')
findPrice = re.compile(r'span class="content__list--item-price"><em>(\d*)</em>')

# 获得对应网页的html字符串
def ask_url(url):
    html = ''
    # 请求头，改为自己的
    header = {
        'User - Agent': 'XXX',
        'Cookie': 'XXX'
    }
    try:
        response = requests.get(url, headers=header)
        html = response.content.decode('utf-8')
        t = uniform(0.5, 1)
        time.sleep(t)
    except requests.exceptions.ProxyError:
        print("。。。。获取网页[%s]的Html文本出错......" % url)

    return html

def get_page(base_url):  # 找出每个区的页数
    page_list = []
    # 正则提取最大页数
    pages = re.compile(r'data-totalpage="(\d*)" data-ur')
    for reg in regions:  # 针对每一个区
        url = base_url + reg + '/pg1/'
        html = ask_url(url)
        soup = BeautifulSoup(html, 'html.parser')
        for item in soup.find_all('div', class_="content__pg"):
            item = str(item)
            # 利用正则提取页数
            total_page = re.findall(pages, item)
            page_list.extend(total_page)
    # 聚合成列表
    # page_list = list(map(int, page_list))
    return page_list

def get_data(base_url, page_list):
    name_list = []  # 房屋名称
    district_list = []  # 房屋所在区
    location_list = []  # 房屋地址
    area_list = []  # 房屋面积
    direction_list = []  # 房屋朝向
    housetype_list = []  # 房型
    subway_list = []  # 是否靠近地铁
    price_list = []  # 房屋月租金

    for i in range(0, len(regions)):  # 外层循环每一个区
        print('*' * 50)
        print('开始爬取%s区域的租房数据......' % regions[i])
        for page in range(1, page_list[i] + 1):  # 总共page页，爬取页面信息page次
            page = page + 1
            url = base_url + regions[i] + '/pg' + str(page)
            print('开始爬取【%s】的租房数据，共【%d】页......' % (url, page_list[i] + 1))

            html = ask_url(url)  # 调用函数ask_url，逐次保存爬取下来的信息
            soup = BeautifulSoup(html, 'html.parser')  # 解析html文件
            try:
                for item in soup.find_all('div', class_="content__list--item"):
                    item = str(item)
                    # 正则匹配找出所需要的信息
                    Name = re.findall(findName, item)[1]
                    District = re.findall(findDistrict, item)[0]
                    Location = re.findall(findLocation, item)[1]
                    Area = re.findall(findArea, item)[0]
                    Direction = re.findall(findDirection, item)[0]
                    Housetype = re.findall(findHousetype, item)[0]
                    Subway = re.findall(findSubway, item)
                    if len(Subway) != 0:
                        Subway = 'yes'
                    else:
                        Subway = 'no'
                    Price = re.findall(findPrice, item)[0]
                    print('【%s--%s--%s--%s--%s--%s--%s--%s】' % (
                        Name, District, Location, Area, Direction, Housetype, Subway, Price))
                    # 添加进list中
                    name_list.append(Name)
                    district_list.append(District)
                    location_list.append(Location)
                    area_list.append(Area)
                    direction_list.append(Direction)
                    housetype_list.append(Housetype)
                    subway_list.append(Subway)
                    price_list.append(Price)
            except:
                print("此条数据出错，已忽略......")

    return (name_list, district_list, location_list, area_list, direction_list, housetype_list, subway_list, price_list)

if __name__ == '__main__':
    base_url = 'https://bj.lianjia.com/zufang/'
    # 得到每个区的页数
    page_list = get_page(base_url=base_url)
    # 得到所有数据
    (
        name_list,
        district_list,
        location_list,
        area_list,
        direction_list,
        housetype_list,
        subway_list,
        price_list

    ) = get_data(base_url=base_url, page_list=page_list)
    # 存入excel表中
    pd.DataFrame({
        'name': name_list,
        'district': district_list,
        'location': location_list,
        'area': area_list,
        'direction': direction_list,
        'housetype': housetype_list,
        'closeToSubway': subway_list,
        'price': price_list
    }).to_excel('北京链家网租房数据.xlsx')

```

以上仅是我一个较为浅薄的理解，如有更好的建议，可以联系我。

